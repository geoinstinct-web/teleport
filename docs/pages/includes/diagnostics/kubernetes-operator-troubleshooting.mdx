The Teleport Operator watches for new resources or changes in Kubernetes.
When a change happens, it triggers the reconciliation loop. This loop is in
charge of validating the resource, checking if it already exists in Teleport
and making calls to the Teleport API to create/update/delete the resource.
The reconciliation loop also adds a `status` field on the Kubernetes resource.

If an error happens and the reconciliation loop is not successful, an item in
`status.conditions` will describe what went wrong. This allows users to diagnose
errors by inspecting Kubernetes resources with `kubectl`:

```code
$ kubectl describe teleportusers myuser
```

For example, if a user has been granted a nonexistent role the status will look like:

```yaml
apiVersion: resources.teleport.dev/v2
kind: TeleportUser
# [...]
status:
  conditions:
  - lastTransitionTime: "2022-07-25T16:15:52Z"
    message: Teleport resource has the Kubernetes origin label.
    reason: OriginLabelMatching
    status: "True"
    type: TeleportResourceOwned
  - lastTransitionTime: "2022-07-25T17:08:58Z"
    message: 'Teleport returned the error: role my-non-existing-role is not found'
    reason: TeleportError
    status: "False"
    type: SuccessfullyReconciled
```

Here `SuccessfullyReconciled` is `False` and the error is `role my-non-existing-role is not found`.

If the status is not present or does not give sufficient information to solve
the issue, check the operator logs:

```shell
$ kubectl logs deploy/<OPERATOR_DEPLOYMENT_NAME>
```

<Admonition type="note">
  In case of multi-replica deployments, only one operator instance is running
  the reconciliation loop. This operator is called the leader and is the only
  one producing reconciliation logs. The other operator instances are waiting
  with the following log:

  ```
  leaderelection.go:248] attempting to acquire leader lease teleport/431e83f4.teleport.dev...
  ```

  To diagnose reconciliation issues, you will have to inspect all pods to find
  the one reconciling the resources.
</Admonition>

If the Kubernetes resource has no status update and the operator does not produce
any logs regarding the resource, please check if the resource lives in the same
namespace as the operator.  The operator only watches for resource in its own namespace.
